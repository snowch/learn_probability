{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12adcf8",
   "metadata": {},
   "source": [
    "# Chapter 5: Bayes' Theorem and Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70f638",
   "metadata": {},
   "source": [
    "In the previous chapter, we explored conditional probability â€“ how the probability of an event changes given that another event has occurred. Now, we'll delve into one of the most powerful and widely applicable results stemming from conditional probability: **Bayes' Theorem**. This theorem provides a formal way to update our beliefs (probabilities) in light of new evidence. We will also formally define and explore the concept of **independence** between events, a crucial idea for simplifying probability calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16db44",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "* Understand the derivation and interpretation of Bayes' Theorem.\n",
    "* Distinguish between prior and posterior probabilities.\n",
    "* Apply Bayes' Theorem to solve problems, particularly diagnostic testing scenarios.\n",
    "* Define and test for the independence of events.\n",
    "* Understand the concept of conditional independence.\n",
    "* Implement Bayesian updates and independence checks using Python simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8b3c0",
   "metadata": {},
   "source": [
    "## 1. Bayes' Theorem: Derivation and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66a040",
   "metadata": {},
   "source": [
    "Bayes' Theorem provides a way to \"reverse\" conditional probabilities. If we know $P(B|A)$, Bayes' Theorem helps us find $P(A|B)$. It's named after Reverend Thomas Bayes (1701-1761), who first provided an equation that allows new evidence to update beliefs.\n",
    "\n",
    "**Derivation:**\n",
    "\n",
    "Recall the definition of conditional probability:\n",
    "\n",
    "1.  $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$, provided $P(B) > 0$.\n",
    "2.  $P(B|A) = \\frac{P(B \\cap A)}{P(A)}$, provided $P(A) > 0$.\n",
    "\n",
    "Since $P(A \\cap B) = P(B \\cap A)$, we can rearrange these equations:\n",
    "\n",
    "1.  $P(A \\cap B) = P(A|B) P(B)$\n",
    "2.  $P(B \\cap A) = P(B|A) P(A)$\n",
    "\n",
    "Setting them equal gives:\n",
    "\n",
    "$P(A|B) P(B) = P(B|A) P(A)$\n",
    "\n",
    "Dividing by $P(B)$ (assuming $P(B) > 0$), we get **Bayes' Theorem**:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$$\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "Let's think of A as an event or hypothesis we are interested in (e.g., \"a patient has a specific disease,\" \"a coin is biased\") and B as new evidence or data observed (e.g., \"the patient tested positive,\" \"we observed 8 heads in 10 flips\").\n",
    "\n",
    "* $P(A)$: **Prior Probability**. Our initial belief about the probability of A *before* seeing the evidence B.\n",
    "* $P(B|A)$: **Likelihood**. The probability of observing the evidence B *given* that our hypothesis A is true.\n",
    "* $P(B)$: **Probability of Evidence**. The overall probability of observing the evidence B, regardless of whether A is true or not. This often requires using the Law of Total Probability (from Chapter 4): $P(B) = P(B|A)P(A) + P(B|A^c)P(A^c)$.\n",
    "* $P(A|B)$: **Posterior Probability**. Our updated belief about the probability of A *after* observing the evidence B.\n",
    "\n",
    "Bayes' Theorem tells us how to update our prior belief $P(A)$ to a posterior belief $P(A|B)$ based on the likelihood of the evidence $P(B|A)$ and the overall probability of the evidence $P(B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423cb4f",
   "metadata": {},
   "source": [
    "## 2. Updating Beliefs: Prior and Posterior Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc86a535",
   "metadata": {},
   "source": [
    "The core idea of Bayesian thinking is updating beliefs. We start with a prior belief, gather data (evidence), and update our belief to a posterior. This posterior can then become the prior for the next piece of evidence.\n",
    "\n",
    "**Example:** Imagine you have a website and you're testing a new ad banner.\n",
    "\n",
    "* **Hypothesis (A):** The new ad banner is effective (e.g., has a click-through rate > 5%).\n",
    "* **Prior ($P(A)$):** Based on previous ad campaigns, you might initially believe there's a 30% chance the new ad is effective. So, $P(A) = 0.30$.\n",
    "* **Evidence (B):** You observe a visitor's Browse history (e.g., they previously visited related product pages).\n",
    "* **Likelihood ($P(B|A)$):** The probability that a visitor has this Browse history *given* the ad is effective. Perhaps effective ads are better targeted, so this might be high, say $P(B|A) = 0.70$.\n",
    "* **Likelihood ($P(B|A^c)$):** The probability that a visitor has this Browse history *given* the ad is *not* effective. This might be lower, say $P(B|A^c) = 0.20$.\n",
    "* **Probability of Evidence ($P(B)$):** Using the Law of Total Probability:\n",
    "    $P(B) = P(B|A)P(A) + P(B|A^c)P(A^c)$\n",
    "    $P(B) = (0.70)(0.30) + (0.20)(1 - 0.30)$\n",
    "    $P(B) = 0.21 + (0.20)(0.70) = 0.21 + 0.14 = 0.35$\n",
    "* **Posterior ($P(A|B)$):** Now apply Bayes' Theorem:\n",
    "    $P(A|B) = \\frac{P(B|A) P(A)}{P(B)} = \\frac{(0.70)(0.30)}{0.35} = \\frac{0.21}{0.35} = 0.60$\n",
    "\n",
    "After observing the visitor's Browse history, your belief that the ad is effective increased from 30% (prior) to 60% (posterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64c11f",
   "metadata": {},
   "source": [
    "## 3. Applications: The Diagnostic Test Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d261e",
   "metadata": {},
   "source": [
    "One of the most classic and intuitive applications of Bayes' Theorem is in interpreting the results of medical diagnostic tests.\n",
    "\n",
    "**Scenario:**\n",
    "* A certain disease affects 1% of the population. (Prevalence)\n",
    "* A test for the disease has 95% accuracy:\n",
    "    * If a person *has* the disease, the test correctly identifies it 95% of the time. (Sensitivity)\n",
    "    * If a person *does not have* the disease, the test correctly identifies it 95% of the time. (Specificity)\n",
    "\n",
    "```{admonition} Sensitivity and Specificity \n",
    ":class: dropdown\n",
    "Looking at the origins and definitions of the words \"sensitivity\" and \"specificity\" can definitely help reinforce their meanings in this context.\n",
    "\n",
    "1. **Sensitivity:**  \n",
    "   * **Origin:** Comes from the Latin word sentire, meaning \"to feel\" or \"to perceive.\"  \n",
    "   * **General Meaning:** The quality or condition of being sensitive; responsiveness to stimuli.  \n",
    "   * **Connection to the Test:** Think of the test as needing to \"feel\" or \"perceive\" the presence of the disease. A highly **sensitive** test has a strong ability to *detect* the disease when it is actually there. It's responsive to the \"stimulus\" of the disease. If the disease is present, a sensitive test is likely to react (give a positive result). This aligns with its technical meaning of correctly identifying true positives.  \n",
    "2. **Specificity:**  \n",
    "   * **Origin:** Comes from the Latin word specificus, derived from species (meaning \"kind\" or \"sort\") and facere (meaning \"to make\"). Essentially, \"making of a particular kind.\"  \n",
    "   * **General Meaning:** The quality of being specific; restricted to a particular item, condition, or effect; being precise or exact.  \n",
    "   * **Connection to the Test:** Think of the test as being designed for one *specific* target â€“ the disease. A highly **specific** test is precise and only reacts to that *particular* target. It does *not* react to other things (like the absence of the disease or other conditions). It correctly identifies individuals who do *not* have the specific target disease (giving a negative result). This aligns with its technical meaning of correctly identifying true negatives.\n",
    "\n",
    "**How it Helps Understanding:**\n",
    "\n",
    "* **Sensitivity:** Relates to the test's ability to **sense** or **detect** the disease if it's present. High sensitivity means good detection.  \n",
    "* **Specificity:** Relates to the test being **specific** or **precise** to only the disease in question. High specificity means the test only flags the *specific* condition it's looking for and avoids flagging healthy people.\n",
    "\n",
    "So, the origins help frame the concepts: sensitivity is about *detection power*, while specificity is about *precision* and *target accuracy*.\n",
    "```\n",
    "\n",
    "**Question:** If a randomly selected person tests positive, what is the probability they actually have the disease?\n",
    "\n",
    "**Let's define the events:**\n",
    "* $D$: The person has the disease.\n",
    "* $D^c$: The person does not have the disease.\n",
    "* $Pos$: The person tests positive.\n",
    "* $Neg$: The person tests negative.\n",
    "\n",
    "**What we know:**\n",
    "* $P(D) = 0.01$ (Prior probability of having the disease - Prevalence)\n",
    "* $P(D^c) = 1 - P(D) = 0.99$\n",
    "* $P(Pos|D) = 0.95$ (Probability of testing positive *given* you have the disease - Sensitivity)\n",
    "* $P(Neg|D) = 1 - P(Pos|D) = 0.05$ (False Negative Rate)\n",
    "* $P(Neg|D^c) = 0.95$ (Probability of testing negative *given* you don't have the disease - Specificity)\n",
    "* $P(Pos|D^c) = 1 - P(Neg|D^c) = 0.05$ (False Positive Rate)\n",
    "\n",
    "**What we want to find:** $P(D|Pos)$ (The probability of having the disease *given* a positive test result).\n",
    "\n",
    "**Apply Bayes' Theorem:**\n",
    "\n",
    "$P(D|Pos) = \\frac{P(Pos|D) P(D)}{P(Pos)}$\n",
    "\n",
    "We need to find $P(Pos)$. Use the Law of Total Probability:\n",
    "$P(Pos) = P(Pos|D)P(D) + P(Pos|D^c)P(D^c)$\n",
    "$P(Pos) = (0.95)(0.01) + (0.05)(0.99)$\n",
    "$P(Pos) = 0.0095 + 0.0495 = 0.0590$\n",
    "\n",
    "Now substitute into Bayes' Theorem:\n",
    "$P(D|Pos) = \\frac{(0.95)(0.01)}{0.0590} = \\frac{0.0095}{0.0590} \\approx 0.161$\n",
    "\n",
    "**Interpretation:** Even with a positive test result from a 95% accurate test, the probability of actually having the disease is only about 16.1%! This seems counter-intuitive but highlights the strong influence of the low prior probability (prevalence) of the disease. Most positive tests come from the large group of healthy people who receive a false positive, rather than the small group of sick people who receive a true positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5eac98",
   "metadata": {},
   "source": [
    "## 4. Independence of Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289680b",
   "metadata": {},
   "source": [
    "Two events A and B are said to be **independent** if the occurrence (or non-occurrence) of one event does not affect the probability of the other event occurring.\n",
    "\n",
    "I.e. Two events A and B are said to be **independent** if knowing whether one event happened tells you nothing about whether the other event will happen. Their probabilities are not linked.\n",
    "\n",
    "### 4.1. Formal Definition\n",
    "\n",
    "The formal mathematical definition of independence between two eventsis that Events A and B are independent if and only if:\n",
    "$P(A \\cap B) = P(A) P(B)$\n",
    "\n",
    "```{admonition} Explanation\n",
    ":class: dropdown\n",
    "\n",
    "Events **A** and **B** are **independent** if and only if the probability that *both* events happen is equal to the product of their individual probabilities.\n",
    "\n",
    "Mathematically:\n",
    "$P(A \\cap B) = P(A) \\times P(B)$\n",
    "\n",
    "* $P(A \\cap B)$ means \"the probability of both A AND B occurring\" (the intersection of A and B).\n",
    "* $P(A)$ is the probability of event A occurring.\n",
    "* $P(B)$ is the probability of event B occurring.\n",
    "\n",
    "**Why does this formula capture independence?**\n",
    "Think about it this way: If the events truly don't influence each other, the chance of them *both* happening should just be a simple multiplication of their individual chances. If there *was* some influence (dependence), this multiplication wouldn't accurately reflect the combined probability.\n",
    "\n",
    "**Example: Flipping a Fair Coin Twice ðŸª™**\n",
    "\n",
    "Let's consider flipping a fair coin two times.\n",
    "\n",
    "* **Event A**: Getting heads (H) on the **first flip**.\n",
    "* **Event B**: Getting heads (H) on the **second flip**.\n",
    "\n",
    "We want to know if these two events are independent.\n",
    "\n",
    "1.  **Calculate $P(A)$**:\n",
    "    The probability of getting heads on a single flip of a fair coin is $\\frac{1}{2}$.\n",
    "    So, $P(A) = \\frac{1}{2}$.\n",
    "\n",
    "2.  **Calculate $P(B)$**:\n",
    "    The outcome of the second flip is not affected by the first flip. The coin has no memory. So, the probability of getting heads on the second flip is also $\\frac{1}{2}$.\n",
    "    So, $P(B) = \\frac{1}{2}$.\n",
    "\n",
    "3.  **Calculate $P(A \\cap B)$**:\n",
    "    This is the probability of getting heads on the first flip **AND** heads on the second flip (HH).\n",
    "    The possible outcomes when flipping a coin twice are: HH, HT, TH, TT. There are 4 equally likely outcomes.\n",
    "    Only one of these outcomes is HH.\n",
    "    So, $P(A \\cap B) = \\frac{1}{4}$.\n",
    "\n",
    "4.  **Check the Independence Formula**:\n",
    "    Now we check if $P(A \\cap B) = P(A) \\times P(B)$.\n",
    "    * $P(A) \\times P(B) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$\n",
    "    * We already found that $P(A \\cap B) = \\frac{1}{4}$.\n",
    "\n",
    "5.  **Conclusion**:\n",
    "    Since $P(A \\cap B) = P(A) \\times P(B)$ (because $\\frac{1}{4} = \\frac{1}{4}$), the events A (heads on the first flip) and B (heads on the second flip) are **independent**.\n",
    "\n",
    "This makes intuitive sense: the result of the first coin flip doesn't change the probability of getting heads or tails on the second flip.\n",
    "```\n",
    "\n",
    "### 4.2. Alternative Definition (using conditional probability)\n",
    "\n",
    "If $P(B) > 0$, A and B are independent if and only if:\n",
    "\n",
    "$P(A|B) = P(A)$\n",
    "\n",
    "Similarly, if $P(A) > 0$, independence means:\n",
    "\n",
    "$P(B|A) = P(B)$\n",
    "\n",
    "This definition aligns with the intuition: knowing B occurred doesn't change the probability of A.\n",
    "\n",
    "```{admonition} Example: Fair Die Roll\n",
    ":class: dropdown\n",
    "\n",
    "| Event Definition                                  | Probability Calculation |\n",
    "| :------------------------------------------------ | :---------------------- |\n",
    "| **A**: \"rolling an even number\" = {2, 4, 6}       | $P(A) = 3/6 = 1/2$      |\n",
    "| **B**: \"rolling a number > 4\" = {5, 6}            | $P(B) = 2/6 = 1/3$      |\n",
    "| **A âˆ© B**: \"even number > 4\" = {6}                | $P(A \\cap B) = 1/6$     |\n",
    "\n",
    "Let's check for independence:\n",
    "\n",
    "Is $P(A \\cap B) = P(A) P(B)$?\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A \\cap B) &\\stackrel{?}{=} P(A) P(B) \\\\\n",
    "\\frac{1}{6} &\\stackrel{?}{=} \\left(\\frac{1}{2}\\right) \\times \\left(\\frac{1}{3}\\right) \\\\\n",
    "\\frac{1}{6} &= \\frac{1}{6} \\quad \\checkmark\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Yes, the events A and B are independent. \n",
    "\n",
    "Knowing the roll is greater than 4 doesn't change the probability that it's even - it's still 1/2: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A|B) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n",
    "&= \\frac{1/6}{1/3} \\\\\n",
    "&= \\frac{1}{6} \\times 3 \\\\\n",
    "&= \\frac{3}{6} \\\\\n",
    "&= \\frac{1}{2} \\\\\n",
    "&= P(A)\n",
    "\\end{align*}\n",
    "$$\n",
    "```\n",
    "\n",
    "```{admonition} Example: Drawing Cards (Without Replacement)\n",
    ":class: dropdown\n",
    "\n",
    "Let A be the event \"the first card drawn is an Ace\". $P(A) = 4/52$.\n",
    "Let B be the event \"the second card drawn is an Ace\".\n",
    "\n",
    "Are A and B independent? Intuitively, no. If the first card was an Ace, the probability the second is an Ace changes.\n",
    "\n",
    "Let's calculate $P(B)$. Using the Law of Total Probability:\n",
    "\n",
    "$P(B) = P(B|A)P(A) + P(B|A^c)P(A^c)$\n",
    "$P(B) = (3/51)(4/52) + (4/51)(48/52) = (12 + 192) / (51 \\times 52) = 204 / 2652 = 4/52$.\n",
    "\n",
    "So, $P(B) = 4/52$.\n",
    "\n",
    "Now let's calculate the intersection: $P(A \\cap B) = P(\\text{first is Ace AND second is Ace})$\n",
    "\n",
    "$P(A \\cap B) = P(B|A)P(A) = (3/51)(4/52) = 12 / 2652 = 1 / 221$.\n",
    "\n",
    "Check for independence: \n",
    "\n",
    "Is $P(A \\cap B) = P(A)P(B)$?\n",
    "$1/221 \\stackrel{?}{=} (4/52) \\times (4/52) = (1/13) \\times (1/13) = 1/169$.\n",
    "$1/221 \\neq 1/169$.\n",
    "\n",
    "As expected, the events are **not** independent.\n",
    "```\n",
    "\n",
    "**Important Note:** Do not confuse independence with mutual exclusivity.\n",
    "* **Mutually exclusive** events cannot happen together ($A \\cap B = \\emptyset$, so $P(A \\cap B) = 0$).\n",
    "* **Independent** events *can* happen together, but one doesn't affect the other's probability.\n",
    "If two events A and B have non-zero probabilities, they *cannot* be both mutually exclusive and independent. If they were mutually exclusive, $P(A \\cap B) = 0$. If they were independent, $P(A \\cap B) = P(A)P(B) > 0$. This is a contradiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2752f0",
   "metadata": {},
   "source": [
    "## 5. Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147e48f",
   "metadata": {},
   "source": [
    "Sometimes, two events A and B might not be independent overall, but they become independent *given* some other event C. This is called **conditional independence**.\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "Events A and B are conditionally independent given event C (where $P(C) > 0$) if:\n",
    "\n",
    "$P(A \\cap B | C) = P(A|C) P(B|C)$\n",
    "\n",
    "**Alternative Definition:**\n",
    "If $P(B|C) > 0$, conditional independence means:\n",
    "\n",
    "$P(A | B \\cap C) = P(A|C)$\n",
    "\n",
    "Knowing B occurred provides no additional information about A *if we already know C occurred*.\n",
    "\n",
    "**Example:** Consider two different coins, one fair (Coin F) and one biased to land heads 75% of the time (Coin B).\n",
    "* Let $H_1$ be the event \"the first flip is Heads\".\n",
    "* Let $H_2$ be the event \"the second flip is Heads\".\n",
    "\n",
    "Are $H_1$ and $H_2$ independent? It depends on whether we know which coin we are flipping!\n",
    "\n",
    "**Scenario 1: We pick a coin at random (50% chance each) and flip it twice.**\n",
    "\n",
    "Let's find $P(H_1)$ and $P(H_2)$.\n",
    "\n",
    "$P(H_1) = P(H_1|\\text{Fair})P(\\text{Fair}) + P(H_1|\\text{Biased})P(\\text{Biased})$\n",
    "\n",
    "$P(H_1) = (0.5)(0.5) + (0.75)(0.5) = 0.25 + 0.375 = 0.625$.\n",
    "\n",
    "By symmetry, $P(H_2) = 0.625$.\n",
    "\n",
    "Now, let's find $P(H_1 \\cap H_2) = P(\\text{HH})$.\n",
    "\n",
    "$P(\\text{HH}) = P(\\text{HH}|\\text{Fair})P(\\text{Fair}) + P(\\text{HH}|\\text{Biased})P(\\text{Biased})$\n",
    "\n",
    "Assuming flips are independent *given* the coin:\n",
    "\n",
    "$P(\\text{HH}) = (0.5 \\times 0.5)(0.5) + (0.75 \\times 0.75)(0.5)$\n",
    "\n",
    "$P(\\text{HH}) = (0.25)(0.5) + (0.5625)(0.5) = 0.125 + 0.28125 = 0.40625$.\n",
    "\n",
    "Check for independence: \n",
    "\n",
    "Is $P(H_1 \\cap H_2) = P(H_1) P(H_2)$?\n",
    "$0.40625 \\stackrel{?}{=} (0.625) \\times (0.625) = 0.390625$.\n",
    "\n",
    "They are **not** equal. $H_1$ and $H_2$ are **not** independent overall. If the first flip is heads, it slightly increases our belief we have the biased coin, thus increasing the probability the second flip is also heads.\n",
    "\n",
    "**Scenario 2: We know we are flipping the Fair coin (Event C = \"Fair coin chosen\").**\n",
    "\n",
    "* $P(H_1 | C) = 0.5$\n",
    "* $P(H_2 | C) = 0.5$\n",
    "* $P(H_1 \\cap H_2 | C) = P(\\text{HH} | \\text{Fair}) = 0.5 \\times 0.5 = 0.25$ (assuming flips are independent for a given coin).\n",
    "\n",
    "Check for conditional independence: Is $P(H_1 \\cap H_2 | C) = P(H_1|C) P(H_2|C)$?\n",
    "$0.25 \\stackrel{?}{=} (0.5) \\times (0.5)$\n",
    "$0.25 = 0.25$. Yes. $H_1$ and $H_2$ are **conditionally independent given** we chose the fair coin.\n",
    "\n",
    "**Scenario 3: We know we are flipping the Biased coin (Event C' = \"Biased coin chosen\").**\n",
    "\n",
    "* $P(H_1 | C') = 0.75$\n",
    "* $P(H_2 | C') = 0.75$\n",
    "* $P(H_1 \\cap H_2 | C') = P(\\text{HH} | \\text{Biased}) = 0.75 \\times 0.75 = 0.5625$.\n",
    "\n",
    "Check for conditional independence: Is $P(H_1 \\cap H_2 | C') = P(H_1|C') P(H_2|C')$?\n",
    "$0.5625 \\stackrel{?}{=} (0.75) \\times (0.75)$\n",
    "$0.5625 = 0.5625$. Yes. $H_1$ and $H_2$ are also **conditionally independent given** we chose the biased coin.\n",
    "\n",
    "**Intuition:** Fuel efficiency might depend on tire pressure and engine size. These two factors might seem correlated overall (cars with bigger engines might tend to have specific tire pressure recommendations). However, *given a specific car model*, the effect of tire pressure on fuel efficiency might be independent of the effect of engine size (assuming the model already fixes the engine size)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20356e6b",
   "metadata": {},
   "source": [
    "## 6. Hands-on Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a45fe6",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Implementing Bayes' Theorem for Disease Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff667b",
   "metadata": {},
   "source": [
    "Let's verify the disease test calculation using Python. Define variables for the prior probability, sensitivity, and specificity, then implement the calculation for $P(D|Pos)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837b22a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior P(Disease): 0.0100\n",
      "Sensitivity P(Pos|Disease): 0.9500\n",
      "Specificity P(Neg|No Disease): 0.9500\n",
      "False Positive Rate P(Pos|No Disease): 0.0500\n",
      "------------------------------\n",
      "Overall P(Pos): 0.0590\n",
      "Posterior P(Disease|Pos): 0.1610\n",
      "------------------------------\n",
      "Overall P(Neg): 0.9410\n",
      "Posterior P(Disease|Neg): 0.0005\n",
      "Posterior P(No Disease|Neg) = 0.9995\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "p_disease = 0.01        # P(D) - Prior probability (prevalence)\n",
    "p_pos_given_disease = 0.95 # P(Pos|D) - Sensitivity\n",
    "p_neg_given_no_disease = 0.95 # P(Neg|D^c) - Specificity\n",
    "\n",
    "# Derived probabilities\n",
    "p_no_disease = 1 - p_disease                 # P(D^c)\n",
    "p_pos_given_no_disease = 1 - p_neg_given_no_disease # P(Pos|D^c) - False Positive Rate\n",
    "\n",
    "# Calculate P(Pos) using Law of Total Probability\n",
    "p_pos = (p_pos_given_disease * p_disease) + (p_pos_given_no_disease * p_no_disease)\n",
    "\n",
    "# Calculate P(D|Pos) using Bayes' Theorem\n",
    "p_disease_given_pos = (p_pos_given_disease * p_disease) / p_pos\n",
    "\n",
    "print(f\"Prior P(Disease): {p_disease:.4f}\")\n",
    "print(f\"Sensitivity P(Pos|Disease): {p_pos_given_disease:.4f}\")\n",
    "print(f\"Specificity P(Neg|No Disease): {p_neg_given_no_disease:.4f}\")\n",
    "print(f\"False Positive Rate P(Pos|No Disease): {p_pos_given_no_disease:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Overall P(Pos): {p_pos:.4f}\")\n",
    "print(f\"Posterior P(Disease|Pos): {p_disease_given_pos:.4f}\")\n",
    "\n",
    "# What if the test is *negative*? Calculate P(Disease | Neg)\n",
    "# P(Neg) = P(Neg|D)P(D) + P(Neg|D^c)P(D^c)\n",
    "p_neg_given_disease = 1 - p_pos_given_disease # P(Neg|D) - False Negative Rate\n",
    "p_neg = (p_neg_given_disease * p_disease) + (p_neg_given_no_disease * p_no_disease)\n",
    "\n",
    "# P(D|Neg) = P(Neg|D)P(D) / P(Neg)\n",
    "p_disease_given_neg = (p_neg_given_disease * p_disease) / p_neg\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Overall P(Neg): {p_neg:.4f}\")\n",
    "print(f\"Posterior P(Disease|Neg): {p_disease_given_neg:.4f}\")\n",
    "print(f\"Posterior P(No Disease|Neg) = {1 - p_disease_given_neg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b02a0",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Simulating Bayesian Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14baf7",
   "metadata": {},
   "source": [
    "Let's simulate the disease test scenario to build intuition. We'll create a population reflecting the disease prevalence, simulate their test results based on sensitivity/specificity, and then calculate the conditional probability directly from the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724dbcbd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Has_Disease  Tests_Positive\n",
      "0        False           False\n",
      "1        False           False\n",
      "2        False           False\n",
      "3        False           False\n",
      "4        False           False\n",
      "\n",
      "--- Simulation Results ---\n",
      "Population Size: 1000000\n",
      "Number Actually Diseased: 10148\n",
      "Number Actually Healthy: 989852\n",
      "Number Testing Positive: 59619\n",
      "  - True Positives: 9629\n",
      "  - False Positives: 49990\n",
      "------------------------------\n",
      "Simulated P(Disease | Positive): 0.1615\n",
      "Theoretical P(Disease | Positive): 0.1610\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "population_size = 1000000\n",
    "p_disease = 0.01\n",
    "p_pos_given_disease = 0.95\n",
    "p_pos_given_no_disease = 0.05 # 1 - specificity\n",
    "\n",
    "# Create population\n",
    "# Assign actual disease status\n",
    "has_disease = np.random.rand(population_size) < p_disease\n",
    "num_diseased = np.sum(has_disease)\n",
    "num_healthy = population_size - num_diseased\n",
    "\n",
    "# Simulate test results\n",
    "# Initialize test results array\n",
    "tests_positive = np.zeros(population_size, dtype=bool)\n",
    "\n",
    "# For those WITH the disease\n",
    "tests_positive[has_disease] = np.random.rand(num_diseased) < p_pos_given_disease\n",
    "\n",
    "# For those WITHOUT the disease\n",
    "tests_positive[~has_disease] = np.random.rand(num_healthy) < p_pos_given_no_disease\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "data = pd.DataFrame({'Has_Disease': has_disease, 'Tests_Positive': tests_positive})\n",
    "print(data.head())\n",
    "\n",
    "# Calculate counts from the simulation\n",
    "true_positives = np.sum(data['Has_Disease'] & data['Tests_Positive'])\n",
    "false_positives = np.sum(~data['Has_Disease'] & data['Tests_Positive'])\n",
    "total_positives = np.sum(data['Tests_Positive'])\n",
    "\n",
    "# Calculate P(Disease | Positive) from simulation data\n",
    "simulated_p_disease_given_pos = true_positives / total_positives\n",
    "\n",
    "# Compare with theoretical calculation\n",
    "print(\"\\n--- Simulation Results ---\")\n",
    "print(f\"Population Size: {population_size}\")\n",
    "print(f\"Number Actually Diseased: {num_diseased}\")\n",
    "print(f\"Number Actually Healthy: {num_healthy}\")\n",
    "print(f\"Number Testing Positive: {total_positives}\")\n",
    "print(f\"  - True Positives: {true_positives}\")\n",
    "print(f\"  - False Positives: {false_positives}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Simulated P(Disease | Positive): {simulated_p_disease_given_pos:.4f}\")\n",
    "print(f\"Theoretical P(Disease | Positive): {p_disease_given_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb483a27",
   "metadata": {},
   "source": [
    "As the `population_size` increases, the simulated probability should converge to the theoretical probability calculated using Bayes' Theorem. This demonstrates how the theorem accurately reflects the underlying frequencies in a large population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b38e0",
   "metadata": {},
   "source": [
    "### Exercise 5.3: Testing Independence from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5fb12",
   "metadata": {},
   "source": [
    "Let's simulate rolling two fair dice and check if the events \"first die is even\" and \"sum is 7\" are independent.\n",
    "\n",
    "* Event A: First die is even. $P(A) = 1/2$.\n",
    "* Event B: Sum is 7. The pairs are (1,6), (2,5), (3,4), (4,3), (5,2), (6,1). $P(B) = 6/36 = 1/6$.\n",
    "* Event $A \\cap B$: First die is even AND sum is 7. The pairs are (2,5), (4,3), (6,1). $P(A \\cap B) = 3/36 = 1/12$.\n",
    "\n",
    "Theoretical Check: Is $P(A \\cap B) = P(A)P(B)$?\n",
    "$1/12 \\stackrel{?}{=} (1/2) \\times (1/6)$\n",
    "$1/12 = 1/12$. Yes, they are theoretically independent.\n",
    "\n",
    "Now, let's check using simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebb69ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Die1  Die2  Sum      A      B\n",
      "0     2     4    6   True  False\n",
      "1     3     6    9  False  False\n",
      "2     2     6    8   True  False\n",
      "3     3     1    4  False  False\n",
      "4     3     5    8  False  False\n",
      "\n",
      "--- Independence Check from Simulation ---\n",
      "Simulated P(A): 0.4994 (Theoretical: 0.5000)\n",
      "Simulated P(B): 0.1655 (Theoretical: 0.1667)\n",
      "Simulated P(A intersect B): 0.0829 (Theoretical: 0.0833)\n",
      "------------------------------\n",
      "P(A) * P(B) = 0.0827\n",
      "Is P(A intersect B) approx equal to P(A) * P(B)? Yes\n",
      "\n",
      "Simulated P(A|B): 0.5011\n",
      "Is P(A|B) approx equal to P(A)? Yes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "num_rolls = 100000\n",
    "\n",
    "# Simulate rolls\n",
    "die1 = np.random.randint(1, 7, size=num_rolls)\n",
    "die2 = np.random.randint(1, 7, size=num_rolls)\n",
    "sums = die1 + die2\n",
    "\n",
    "# Define events\n",
    "event_A = (die1 % 2 == 0)  # First die is even\n",
    "event_B = (sums == 7)     # Sum is 7\n",
    "\n",
    "# Create DataFrame\n",
    "rolls_data = pd.DataFrame({'Die1': die1, 'Die2': die2, 'Sum': sums, 'A': event_A, 'B': event_B})\n",
    "print(rolls_data.head())\n",
    "\n",
    "# Calculate probabilities from simulation\n",
    "p_A_sim = np.mean(event_A)\n",
    "p_B_sim = np.mean(event_B)\n",
    "p_A_intersect_B_sim = np.mean(event_A & event_B)\n",
    "\n",
    "# Check independence condition\n",
    "print(\"\\n--- Independence Check from Simulation ---\")\n",
    "print(f\"Simulated P(A): {p_A_sim:.4f} (Theoretical: 0.5000)\")\n",
    "print(f\"Simulated P(B): {p_B_sim:.4f} (Theoretical: {1/6:.4f})\")\n",
    "print(f\"Simulated P(A intersect B): {p_A_intersect_B_sim:.4f} (Theoretical: {1/12:.4f})\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"P(A) * P(B) = {p_A_sim * p_B_sim:.4f}\")\n",
    "print(f\"Is P(A intersect B) approx equal to P(A) * P(B)? {'Yes' if np.isclose(p_A_intersect_B_sim, p_A_sim * p_B_sim, atol=0.005) else 'No'}\") # Use np.isclose for floating point comparison\n",
    "\n",
    "# Alternative check: Is P(A|B) approx equal to P(A)?\n",
    "# P(A|B) = P(A intersect B) / P(B)\n",
    "if p_B_sim > 0:\n",
    "    p_A_given_B_sim = p_A_intersect_B_sim / p_B_sim\n",
    "    print(f\"\\nSimulated P(A|B): {p_A_given_B_sim:.4f}\")\n",
    "    print(f\"Is P(A|B) approx equal to P(A)? {'Yes' if np.isclose(p_A_given_B_sim, p_A_sim, atol=0.01) else 'No'}\")\n",
    "else:\n",
    "    print(\"\\nCannot calculate P(A|B) as P(B) is zero in simulation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef503935",
   "metadata": {},
   "source": [
    "The simulation results should be close to the theoretical values, confirming the independence of these events. Small discrepancies are expected due to random sampling variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6f0b1",
   "metadata": {},
   "source": [
    "## Chapter Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d73f2c",
   "metadata": {},
   "source": [
    "* **Bayes' Theorem** $P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$ provides a fundamental rule for updating probabilities (beliefs) based on new evidence.\n",
    "* It relates the **posterior probability** $P(A|B)$ to the **prior probability** $P(A)$ and the **likelihood** $P(B|A)$.\n",
    "* The term $P(B)$ acts as a normalizing constant and can often be calculated using the **Law of Total Probability**.\n",
    "* Bayes' Theorem is crucial in fields like medical diagnosis, machine learning (spam filtering, classification), and scientific reasoning.\n",
    "* Two events A and B are **independent** if $P(A \\cap B) = P(A)P(B)$, or equivalently, $P(A|B) = P(A)$ (assuming $P(B)>0$). The occurrence of one does not change the probability of the other.\n",
    "* Events A and B are **conditionally independent** given C if $P(A \\cap B | C) = P(A|C)P(B|C)$. They become independent once the outcome of C is known.\n",
    "* Simulation is a valuable tool for building intuition about Bayes' Theorem and independence by observing frequencies in generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285a83f",
   "metadata": {},
   "source": [
    "In the next part of the book, we will shift our focus from events to **Random Variables** â€“ numerical outcomes of random phenomena â€“ and explore their distributions. This will allow us to model and analyze probabilistic situations in a more structured way."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
