
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 11: Independence, Covariance, and Correlation &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_11';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Chapter 12: Functions of Multiple Random Variables" href="chapter_12.html" />
    <link rel="prev" title="Chapter 10: Joint Distributions" href="chapter_10.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="index.html">
                    Probability in Practice: A Hands-On Journey with Python
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preface</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="_preface.html">Preface</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1 - Foundations of Probability</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter_01.html">Chapter 1: Introduction to Probability and Python Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_02.html">Chapter 2: The Language of Probability: Sets, Sample Spaces, and Events</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_03.html">Chapter 3: Counting Techniques: Permutations and Combinations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2 - Conditional Probability and Independence</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter_04.html">Chapter 4: Conditional Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_05.html">Chapter 5: Bayes’ Theorem and Independence</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3 - Random Variables and Distributions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter_06.html">Chapter 6: Discrete Random Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_07.html">Chapter 7: Common Discrete Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_08.html">Chapter 8: Continuous Random Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_09.html">Chapter 9: Common Continuous Distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 4 - Multiple Random Variables</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter_10.html">Chapter 10: Joint Distributions</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 11: Independence, Covariance, and Correlation</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_12.html">Chapter 12: Functions of Multiple Random Variables</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 5 - Limit Theorems and Their Significance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter_13.html">Chapter 13: The Law of Large Numbers (LLN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_14.html">Chapter 14: The Central Limit Theorem (CLT)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 6 - Advanced Topics and Applications</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter_15.html">Chapter 15: Introduction to Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_16.html">Chapter 16: Introduction to Markov Chains</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_17.html">Chapter 17: Monte Carlo Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_18.html">Chapter 18: (Optional) Further Explorations</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapter_11.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter_11.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 11: Independence, Covariance, and Correlation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-of-random-variables">Independence of Random Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-for-independence">Checking for Independence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-covariance-with-numpy">Calculating Covariance with NumPy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-coefficient">Correlation Coefficient</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-and-visualizing-correlation">Calculating and Visualizing Correlation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-correlation-between-study-hours-and-exam-scores">Example: Correlation between Study Hours and Exam Scores</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-of-sums-of-random-variables">Variance of Sums of Random Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hands-on-demonstrating-variance-rules-via-simulation">Hands-on: Demonstrating Variance Rules via Simulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-11-independence-covariance-and-correlation">
<h1>Chapter 11: Independence, Covariance, and Correlation<a class="headerlink" href="#chapter-11-independence-covariance-and-correlation" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>In the previous chapter, we explored how to describe the probability distributions of multiple random variables simultaneously using joint distributions. We saw how to derive marginal and conditional distributions from the joint distribution. Now, we delve deeper into the relationships <em>between</em> random variables.</p>
<p>How can we quantify if knowing the value of one variable tells us something about the other? Are they completely unrelated (independent)? Or do they tend to move together or in opposite directions? This chapter introduces three fundamental concepts for describing these relationships:</p>
<ol class="arabic simple">
<li><p><strong>Independence:</strong> The strongest form of “unrelatedness,” where the value of one variable provides no information about the value of the other.</p></li>
<li><p><strong>Covariance:</strong> A measure of the <em>direction</em> of the linear relationship between two variables.</p></li>
<li><p><strong>Correlation:</strong> A <em>standardized</em> measure of the <em>strength and direction</em> of the linear relationship between two variables.</p></li>
</ol>
<p>We will also explore how these concepts affect the variance of sums of random variables, a crucial calculation in fields like finance (portfolio variance) and engineering (error propagation).</p>
<p><strong>Learning Objectives:</strong></p>
<ul class="simple">
<li><p>Understand the definition and implications of independence for random variables.</p></li>
<li><p>Define, calculate, and interpret covariance.</p></li>
<li><p>Define, calculate, interpret, and understand the properties of the correlation coefficient.</p></li>
<li><p>Learn how to calculate the variance of sums of random variables, considering their covariance.</p></li>
<li><p>Apply these concepts using Python (NumPy, Pandas, Matplotlib/Seaborn) for simulation, calculation, and visualization.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">binom</span><span class="p">,</span> <span class="n">uniform</span>

<span class="c1"># Set default plot style</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-darkgrid&#39;</span><span class="p">)</span>
<span class="c1"># Set seed for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">4</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">binom</span><span class="p">,</span> <span class="n">uniform</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;matplotlib&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="independence-of-random-variables">
<h2>Independence of Random Variables<a class="headerlink" href="#independence-of-random-variables" title="Link to this heading">#</a></h2>
<p>Recall from Chapter 5 that two events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent if <span class="math notranslate nohighlight">\(P(A \cap B) = P(A)P(B)\)</span>. We extend this concept to random variables.</p>
<p><strong>Definition:</strong> Two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>independent</strong> if for <em>any</em> sets <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> (in the range of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, respectively), the events <span class="math notranslate nohighlight">\(\{X \in A\}\)</span> and <span class="math notranslate nohighlight">\(\{Y \in B\}\)</span> are independent. That is:</p>
<div class="math notranslate nohighlight">
\[ P(X \in A, Y \in B) = P(X \in A) P(Y \in B) \]</div>
<p>This is equivalent to saying that their joint distribution function factors into the product of their marginal distribution functions:</p>
<ul class="simple">
<li><p><strong>Discrete:</strong> <span class="math notranslate nohighlight">\(P(X=x, Y=y) = P(X=x) P(Y=y)\)</span> for all possible values <span class="math notranslate nohighlight">\(x, y\)</span>. (Joint PMF = Product of Marginal PMFs)</p></li>
<li><p><strong>Continuous:</strong> <span class="math notranslate nohighlight">\(f_{X,Y}(x,y) = f_X(x) f_Y(y)\)</span> for all <span class="math notranslate nohighlight">\(x, y\)</span>. (Joint PDF = Product of Marginal PDFs)</p></li>
</ul>
<p><strong>Intuition:</strong> If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, knowing the outcome of <span class="math notranslate nohighlight">\(X\)</span> provides no information about the outcome of <span class="math notranslate nohighlight">\(Y\)</span>, and vice-versa.</p>
<p><strong>Example:</strong></p>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(X\)</span> be the outcome of a fair coin flip (0 for Tails, 1 for Heads). <span class="math notranslate nohighlight">\(P(X=0)=0.5, P(X=1)=0.5\)</span>.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(Y\)</span> be the outcome of a fair six-sided die roll ({1, 2, 3, 4, 5, 6}). <span class="math notranslate nohighlight">\(P(Y=y)=1/6\)</span> for <span class="math notranslate nohighlight">\(y \in \{1, ..., 6\}\)</span>.
Assuming the flip and the roll don’t influence each other, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent. The probability of getting Heads (<span class="math notranslate nohighlight">\(X=1\)</span>) and rolling a 4 (<span class="math notranslate nohighlight">\(Y=4\)</span>) is:
<span class="math notranslate nohighlight">\(P(X=1, Y=4) = P(X=1)P(Y=4) = (0.5) \times (1/6) = 1/12\)</span>.</p></li>
</ul>
<p><strong>Non-Example:</strong></p>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(H\)</span> be a person’s height and <span class="math notranslate nohighlight">\(W\)</span> be their weight. Intuitively, taller people tend to weigh more. Knowing someone is very tall (<span class="math notranslate nohighlight">\(H\)</span> is large) makes it more likely their weight (<span class="math notranslate nohighlight">\(W\)</span>) is also large. Therefore, <span class="math notranslate nohighlight">\(H\)</span> and <span class="math notranslate nohighlight">\(W\)</span> are generally <em>not</em> independent. We wouldn’t expect <span class="math notranslate nohighlight">\(f_{H,W}(h,w) = f_H(h) f_W(w)\)</span>.</p></li>
</ul>
<section id="checking-for-independence">
<h3>Checking for Independence<a class="headerlink" href="#checking-for-independence" title="Link to this heading">#</a></h3>
<p>In practice, we often <em>assume</em> independence based on the physical nature of the processes generating the random variables (like separate coin flips). If we have the joint distribution, we can check if it factorizes into the product of the marginals. If we only have data, testing for independence rigorously is complex (involving statistical hypothesis tests beyond the scope of basic probability). However, we’ll see soon that calculating the <em>correlation</em> can give us a clue (if correlation is non-zero, they are dependent; if correlation is zero, they <em>might</em> be independent).</p>
</section>
</section>
<section id="covariance">
<h2>Covariance<a class="headerlink" href="#covariance" title="Link to this heading">#</a></h2>
<p>If variables are not independent, they are dependent. Covariance is a measure that describes the <em>direction</em> of the linear relationship between two random variables.</p>
<p><strong>Definition:</strong> The <strong>covariance</strong> between two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, denoted <span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y)\)</span> or <span class="math notranslate nohighlight">\(\sigma_{XY}\)</span>, is:</p>
<div class="math notranslate nohighlight">
\[ \mathrm{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] \]</div>
<p>This formula calculates the expected value of the product of the deviations of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> from their respective means.</p>
<p>A more convenient formula for calculation is often:</p>
<div class="math notranslate nohighlight">
\[ \mathrm{Cov}(X, Y) = E[XY] - E[X]E[Y] \]</div>
<p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) &gt; 0\)</span></strong>: Indicates a <em>positive linear relationship</em>. When <span class="math notranslate nohighlight">\(X\)</span> is above its mean, <span class="math notranslate nohighlight">\(Y\)</span> tends to be above its mean, and vice-versa. (Example: Height and Weight).</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) &lt; 0\)</span></strong>: Indicates a <em>negative linear relationship</em>. When <span class="math notranslate nohighlight">\(X\)</span> is above its mean, <span class="math notranslate nohighlight">\(Y\)</span> tends to be below its mean, and vice-versa. (Example: Temperature and Heating Costs).</p></li>
<li><p><strong><span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) = 0\)</span></strong>: Indicates <em>no linear relationship</em>. This is a necessary condition for independence, but not sufficient (more on this later).</p></li>
</ul>
<p><strong>Properties:</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathrm{Cov}(X, X) = E[(X - E[X])^2] = \mathrm{Var}(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) = \mathrm{Cov}(Y, X)\)</span> (Symmetric)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Cov}(aX + b, cY + d) = ac \mathrm{Cov}(X, Y)\)</span> for constants <span class="math notranslate nohighlight">\(a, b, c, d\)</span>. (Scaling affects covariance)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Cov}(X+Y, Z) = \mathrm{Cov}(X, Z) + \mathrm{Cov}(Y, Z)\)</span> (Distributive)</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, then <span class="math notranslate nohighlight">\(E[XY] = E[X]E[Y]\)</span>, which implies <span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) = 0\)</span>.</p></li>
</ol>
<p><strong>Important Note:</strong> The value of covariance depends on the units of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. For example, <span class="math notranslate nohighlight">\(\mathrm{Cov}(\text{Height in cm}, \text{Weight in kg})\)</span> will be much larger than <span class="math notranslate nohighlight">\(\mathrm{Cov}(\text{Height in m}, \text{Weight in kg})\)</span>, even though the underlying relationship is the same. This makes it hard to judge the <em>strength</em> of the relationship from covariance alone.</p>
<section id="calculating-covariance-with-numpy">
<h3>Calculating Covariance with NumPy<a class="headerlink" href="#calculating-covariance-with-numpy" title="Link to this heading">#</a></h3>
<p>NumPy’s <code class="docutils literal notranslate"><span class="pre">np.cov()</span></code> function calculates the covariance matrix. For two 1D arrays <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">np.cov(x,</span> <span class="pre">y)</span></code> returns a 2x2 matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\mathrm{Var}(X) &amp; \mathrm{Cov}(X, Y) \\
\mathrm{Cov}(Y, X) &amp; \mathrm{Var}(Y)
\end{pmatrix}
\end{split}\]</div>
<p>We are usually interested in the off-diagonal elements, <span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate two potentially related variables</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="c1"># X: Standard normal</span>
<span class="n">x_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="c1"># Y: Linearly related to X plus some noise</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>
<span class="n">y_samples_pos</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x_samples</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">noise</span> <span class="c1"># Positive relationship</span>
<span class="n">y_samples_neg</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">x_samples</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">noise</span> <span class="c1"># Negative relationship</span>
<span class="n">y_samples_indep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="c1"># Independent</span>

<span class="c1"># Calculate covariance matrices</span>
<span class="n">cov_matrix_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_samples</span><span class="p">,</span> <span class="n">y_samples_pos</span><span class="p">)</span>
<span class="n">cov_matrix_neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_samples</span><span class="p">,</span> <span class="n">y_samples_neg</span><span class="p">)</span>
<span class="n">cov_matrix_indep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">x_samples</span><span class="p">,</span> <span class="n">y_samples_indep</span><span class="p">)</span>

<span class="c1"># Extract the covariance values</span>
<span class="n">cov_xy_pos</span> <span class="o">=</span> <span class="n">cov_matrix_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">cov_xy_neg</span> <span class="o">=</span> <span class="n">cov_matrix_neg</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">cov_xy_indep</span> <span class="o">=</span> <span class="n">cov_matrix_indep</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Covariance Matrix (X, Y_pos):</span><span class="se">\n</span><span class="si">{</span><span class="n">cov_matrix_pos</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cov(X, Y_pos): </span><span class="si">{</span><span class="n">cov_xy_pos</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cov(X, Y_neg): </span><span class="si">{</span><span class="n">cov_xy_neg</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cov(X, Y_indep): </span><span class="si">{</span><span class="n">cov_xy_indep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance Matrix (X, Y_pos):
[[0.95886385 1.89799796]
 [1.89799796 4.00526524]]

Cov(X, Y_pos): 1.8980
Cov(X, Y_neg): -1.4580
Cov(X, Y_indep): 0.0213
</pre></div>
</div>
</div>
</div>
<p>As expected:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Cov(X,</span> <span class="pre">Y_pos)</span></code> is positive, indicating they tend to increase together.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Cov(X,</span> <span class="pre">Y_neg)</span></code> is negative, indicating when one increases, the other tends to decrease.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Cov(X,</span> <span class="pre">Y_indep)</span></code> is close to zero, consistent with independence (or at least no linear relationship). The small non-zero value is due to random sampling variability.</p></li>
</ul>
</section>
</section>
<section id="correlation-coefficient">
<h2>Correlation Coefficient<a class="headerlink" href="#correlation-coefficient" title="Link to this heading">#</a></h2>
<p>To overcome the unit-dependency of covariance and get a measure of the <em>strength</em> of the linear relationship, we use the <strong>correlation coefficient</strong>.</p>
<p><strong>Definition:</strong> The Pearson correlation coefficient between two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, denoted <span class="math notranslate nohighlight">\(\rho(X, Y)\)</span>, <span class="math notranslate nohighlight">\(\rho_{XY}\)</span>, or sometimes <span class="math notranslate nohighlight">\(\mathrm{Corr}(X,Y)\)</span>, is defined as:</p>
<div class="math notranslate nohighlight">
\[ \rho(X, Y) = \frac{\mathrm{Cov}(X, Y)}{\sigma_X \sigma_Y} = \frac{\mathrm{Cov}(X, Y)}{\sqrt{\mathrm{Var}(X) \mathrm{Var}(Y)}} \]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_X\)</span> and <span class="math notranslate nohighlight">\(\sigma_Y\)</span> are the standard deviations of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, assuming they are non-zero.</p>
<p><strong>Properties:</strong></p>
<ol class="arabic simple">
<li><p><strong>Range:</strong> <span class="math notranslate nohighlight">\(-1 \le \rho(X, Y) \le 1\)</span>. The correlation coefficient is dimensionless.</p></li>
<li><p><strong>Linear Relationship:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\rho(X, Y) = 1\)</span>: Perfect positive linear relationship (<span class="math notranslate nohighlight">\(Y = aX + b\)</span> with <span class="math notranslate nohighlight">\(a &gt; 0\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho(X, Y) = -1\)</span>: Perfect negative linear relationship (<span class="math notranslate nohighlight">\(Y = aX + b\)</span> with <span class="math notranslate nohighlight">\(a &lt; 0\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\rho(X, Y) = 0\)</span>: No <em>linear</em> relationship.</p></li>
</ul>
</li>
<li><p><strong>Symmetry:</strong> <span class="math notranslate nohighlight">\(\rho(X, Y) = \rho(Y, X)\)</span>.</p></li>
<li><p><strong>Invariance to Linear Transformation:</strong> <span class="math notranslate nohighlight">\(\rho(aX + b, cY + d) = \mathrm{sign}(ac) \rho(X, Y)\)</span>, assuming <span class="math notranslate nohighlight">\(a \ne 0, c \ne 0\)</span>. Scaling and shifting variables doesn’t change the magnitude of the correlation, only potentially the sign.</p></li>
<li><p><strong>Independence Implies Zero Correlation:</strong> If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, then <span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) = 0\)</span>, which means <span class="math notranslate nohighlight">\(\rho(X, Y) = 0\)</span>.</p></li>
</ol>
<p><strong>Crucial Warning: Correlation does not imply causation!</strong> Just because two variables are correlated doesn’t mean one causes the other. There might be a lurking (confounding) variable influencing both. (Classic example: Ice cream sales and crime rates are correlated, but both are caused by warmer weather).</p>
<p><strong>Crucial Warning 2: Zero correlation does not imply independence!</strong> Correlation measures only <em>linear</em> dependence. It’s possible for <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> to be strongly dependent in a non-linear way, yet have zero correlation.</p>
<p>Consider <span class="math notranslate nohighlight">\(X \sim \text{Uniform}(-1, 1)\)</span> and <span class="math notranslate nohighlight">\(Y = X^2\)</span>. Clearly, <span class="math notranslate nohighlight">\(Y\)</span> is perfectly dependent on <span class="math notranslate nohighlight">\(X\)</span>.
<span class="math notranslate nohighlight">\(E[X] = 0\)</span>. <span class="math notranslate nohighlight">\(E[Y] = E[X^2] = \int_{-1}^{1} x^2 (1/2) dx = [x^3/6]_{-1}^1 = 1/3\)</span>.
<span class="math notranslate nohighlight">\(E[XY] = E[X^3] = \int_{-1}^{1} x^3 (1/2) dx = [x^4/8]_{-1}^1 = 0\)</span>.
<span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) = E[XY] - E[X]E[Y] = 0 - (0)(1/3) = 0\)</span>.
Thus, <span class="math notranslate nohighlight">\(\rho(X, Y) = 0\)</span>, even though <span class="math notranslate nohighlight">\(Y\)</span> is completely determined by <span class="math notranslate nohighlight">\(X\)</span>.</p>
<section id="calculating-and-visualizing-correlation">
<h3>Calculating and Visualizing Correlation<a class="headerlink" href="#calculating-and-visualizing-correlation" title="Link to this heading">#</a></h3>
<p>We can use <code class="docutils literal notranslate"><span class="pre">np.corrcoef()</span></code> or Pandas DataFrame’s <code class="docutils literal notranslate"><span class="pre">.corr()</span></code> method. Scatter plots are essential for visualizing the relationship.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate correlation coefficients using the same samples</span>
<span class="n">corr_matrix_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">x_samples</span><span class="p">,</span> <span class="n">y_samples_pos</span><span class="p">)</span>
<span class="n">corr_matrix_neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">x_samples</span><span class="p">,</span> <span class="n">y_samples_neg</span><span class="p">)</span>
<span class="n">corr_matrix_indep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">x_samples</span><span class="p">,</span> <span class="n">y_samples_indep</span><span class="p">)</span>

<span class="c1"># Extract correlation values</span>
<span class="n">corr_xy_pos</span> <span class="o">=</span> <span class="n">corr_matrix_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">corr_xy_neg</span> <span class="o">=</span> <span class="n">corr_matrix_neg</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">corr_xy_indep</span> <span class="o">=</span> <span class="n">corr_matrix_indep</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correlation Matrix (X, Y_pos):</span><span class="se">\n</span><span class="si">{</span><span class="n">corr_matrix_pos</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Corr(X, Y_pos): </span><span class="si">{</span><span class="n">corr_xy_pos</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Corr(X, Y_neg): </span><span class="si">{</span><span class="n">corr_xy_neg</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Corr(X, Y_indep): </span><span class="si">{</span><span class="n">corr_xy_indep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualize the relationships</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_samples</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_samples_pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Positive Correlation (rho approx </span><span class="si">{</span><span class="n">corr_xy_pos</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">$)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y_pos&#39;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_samples</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_samples_neg</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Negative Correlation (rho approx </span><span class="si">{</span><span class="n">corr_xy_neg</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">$)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y_neg&#39;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_samples</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_samples_indep</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Near Zero Correlation (rho approx </span><span class="si">{</span><span class="n">corr_xy_indep</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">$)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Y_indep&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Correlation Matrix (X, Y_pos):
[[1.         0.96850446]
 [0.96850446 1.        ]]

Corr(X, Y_pos): 0.9685
Corr(X, Y_neg): -0.9483
Corr(X, Y_indep): 0.0221
</pre></div>
</div>
<img alt="_images/fbdef5cbac7994cfdaf749ba3d85c35b372421f9aff1356861852aef34b7f9ff.png" src="_images/fbdef5cbac7994cfdaf749ba3d85c35b372421f9aff1356861852aef34b7f9ff.png" />
</div>
</div>
<p>The scatter plots visually confirm the relationships indicated by the correlation coefficients. Note how the spread around the potential line affects the magnitude of <span class="math notranslate nohighlight">\(\rho\)</span>. <code class="docutils literal notranslate"><span class="pre">Y_pos</span></code> has a stronger linear relationship (less noise relative to the slope) than <code class="docutils literal notranslate"><span class="pre">Y_neg</span></code> in our simulation, resulting in <span class="math notranslate nohighlight">\(|\rho_{XY_{pos}}| &gt; |\rho_{XY_{neg}}|\)</span>. The independent case shows no discernible linear pattern.</p>
<section id="example-correlation-between-study-hours-and-exam-scores">
<h4>Example: Correlation between Study Hours and Exam Scores<a class="headerlink" href="#example-correlation-between-study-hours-and-exam-scores" title="Link to this heading">#</a></h4>
<p>Let’s simulate some data where exam scores depend linearly on study hours, but with some random variation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_students</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1"># Study hours: Uniformly distributed between 1 and 10 hours/week</span>
<span class="n">study_hours</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n_students</span><span class="p">)</span>
<span class="c1"># Exam score: Base score + hours effect + random noise (normal)</span>
<span class="n">base_score</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">hours_effect</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">study_hours</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">n_students</span><span class="p">)</span> <span class="c1"># Std dev of 8 points</span>
<span class="n">exam_scores</span> <span class="o">=</span> <span class="n">base_score</span> <span class="o">+</span> <span class="n">hours_effect</span> <span class="o">+</span> <span class="n">noise</span>
<span class="c1"># Ensure scores are within a reasonable range (e.g., 0-100)</span>
<span class="n">exam_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">exam_scores</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># Calculate Covariance and Correlation</span>
<span class="n">cov_study_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">study_hours</span><span class="p">,</span> <span class="n">exam_scores</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">corr_study_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">study_hours</span><span class="p">,</span> <span class="n">exam_scores</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Covariance between Study Hours and Exam Score: </span><span class="si">{</span><span class="n">cov_study_score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Correlation between Study Hours and Exam Score: </span><span class="si">{</span><span class="n">corr_study_score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">study_hours</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">exam_scores</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Study Hours vs Exam Score (rho approx </span><span class="si">{</span><span class="n">corr_study_score</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">$)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Study Hours per Week&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Exam Score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Covariance between Study Hours and Exam Score: 29.70
Correlation between Study Hours and Exam Score: 0.85
</pre></div>
</div>
<img alt="_images/e471f44138e9d8d36094d42dd22792e3fa919ea4d4c276cd5d60315166ab894d.png" src="_images/e471f44138e9d8d36094d42dd22792e3fa919ea4d4c276cd5d60315166ab894d.png" />
</div>
</div>
<p>The positive correlation confirms the simulated relationship: students who study more tend to get higher scores. The correlation isn’t 1 because of the random noise component (representing other factors like innate ability, test anxiety, luck).</p>
</section>
</section>
</section>
<section id="variance-of-sums-of-random-variables">
<h2>Variance of Sums of Random Variables<a class="headerlink" href="#variance-of-sums-of-random-variables" title="Link to this heading">#</a></h2>
<p>Knowing the covariance or correlation is crucial when calculating the variance of a sum or difference of random variables.</p>
<p><strong>Theorem:</strong> For any two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, and constants <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}(aX + bY) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2ab \mathrm{Cov}(X, Y) \]</div>
<p><strong>Proof Sketch:</strong>
<span class="math notranslate nohighlight">\(\mathrm{Var}(aX + bY) = E[((aX + bY) - E[aX + bY])^2]\)</span>
<span class="math notranslate nohighlight">\(= E[(a(X - E[X]) + b(Y - E[Y]))^2]\)</span>
<span class="math notranslate nohighlight">\(= E[a^2(X - E[X])^2 + b^2(Y - E[Y])^2 + 2ab(X - E[X])(Y - E[Y])]\)</span>
Use linearity of expectation:
<span class="math notranslate nohighlight">\(= a^2 E[(X - E[X])^2] + b^2 E[(Y - E[Y])^2] + 2ab E[(X - E[X])(Y - E[Y])]\)</span>
<span class="math notranslate nohighlight">\(= a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2ab \mathrm{Cov}(X, Y)\)</span></p>
<p><strong>Special Case: Sum of Variables (<span class="math notranslate nohighlight">\(a=1, b=1\)</span>)</strong></p>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2 \mathrm{Cov}(X, Y) \]</div>
<p><strong>Special Case: Difference of Variables (<span class="math notranslate nohighlight">\(a=1, b=-1\)</span>)</strong></p>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}(X - Y) = \mathrm{Var}(X) + (-1)^2 \mathrm{Var}(Y) + 2(1)(-1) \mathrm{Cov}(X, Y) \]</div>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}(X - Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) - 2 \mathrm{Cov}(X, Y) \]</div>
<p><strong>Crucial Simplification: Independent Variables</strong></p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>independent</strong>, then <span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) = 0\)</span>. The formulas simplify significantly:</p>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}(aX + bY) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) \quad (\text{if independent}) \]</div>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) \quad (\text{if independent}) \]</div>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}(X - Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) \quad (\text{if independent}) \]</div>
<p><strong>Extension to Multiple Variables:</strong>
For <span class="math notranslate nohighlight">\(n\)</span> random variables <span class="math notranslate nohighlight">\(X_1, X_2, ..., X_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 \mathrm{Var}(X_i) + \sum_{i \ne j} a_i a_j \mathrm{Cov}(X_i, X_j) \]</div>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 \mathrm{Var}(X_i) + 2 \sum_{i &lt; j} a_i a_j \mathrm{Cov}(X_i, X_j) \]</div>
<p>If all <span class="math notranslate nohighlight">\(X_i\)</span> are independent, then all <span class="math notranslate nohighlight">\(\mathrm{Cov}(X_i, X_j) = 0\)</span> for <span class="math notranslate nohighlight">\(i \ne j\)</span>, and:</p>
<div class="math notranslate nohighlight">
\[ \mathrm{Var}\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 \mathrm{Var}(X_i) \quad (\text{if independent}) \]</div>
<p><strong>Example Application: Portfolio Variance</strong>
Imagine a simple portfolio with investment <span class="math notranslate nohighlight">\(a\)</span> in Stock A (return <span class="math notranslate nohighlight">\(X\)</span>) and investment <span class="math notranslate nohighlight">\(b\)</span> in Stock B (return <span class="math notranslate nohighlight">\(Y\)</span>). The total return is <span class="math notranslate nohighlight">\(R = aX + bY\)</span>. The risk (variance) of the portfolio is:
<span class="math notranslate nohighlight">\(\mathrm{Var}(R) = a^2 \mathrm{Var}(X) + b^2 \mathrm{Var}(Y) + 2ab \mathrm{Cov}(X, Y)\)</span>.
If the stocks are negatively correlated (<span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) &lt; 0\)</span>), the portfolio variance is <em>reduced</em> compared to holding uncorrelated assets. This is the principle of diversification.</p>
<section id="hands-on-demonstrating-variance-rules-via-simulation">
<h3>Hands-on: Demonstrating Variance Rules via Simulation<a class="headerlink" href="#hands-on-demonstrating-variance-rules-via-simulation" title="Link to this heading">#</a></h3>
<p>Let’s simulate and verify the variance rules.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Case 1: Independent Variables</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">X_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span> <span class="c1"># Mean=5, Var=4</span>
<span class="n">Y_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span> <span class="c1"># Mean=10, Var=9</span>

<span class="c1"># Theoretical values</span>
<span class="n">var_X_th</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">var_Y_th</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">cov_XY_th_ind</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Because independent</span>
<span class="n">var_sum_th_ind</span> <span class="o">=</span> <span class="n">var_X_th</span> <span class="o">+</span> <span class="n">var_Y_th</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">cov_XY_th_ind</span>
<span class="n">var_diff_th_ind</span> <span class="o">=</span> <span class="n">var_X_th</span> <span class="o">+</span> <span class="n">var_Y_th</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">cov_XY_th_ind</span>

<span class="c1"># Empirical values from simulation</span>
<span class="n">var_X_emp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X_ind</span><span class="p">)</span>
<span class="n">var_Y_emp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">Y_ind</span><span class="p">)</span>
<span class="n">cov_XY_emp_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_ind</span><span class="p">,</span> <span class="n">Y_ind</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">var_sum_emp_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X_ind</span> <span class="o">+</span> <span class="n">Y_ind</span><span class="p">)</span>
<span class="n">var_diff_emp_ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X_ind</span> <span class="o">-</span> <span class="n">Y_ind</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- Independent Case ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Var(X): </span><span class="si">{</span><span class="n">var_X_th</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Empirical Var(X): </span><span class="si">{</span><span class="n">var_X_emp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Var(Y): </span><span class="si">{</span><span class="n">var_Y_th</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Empirical Var(Y): </span><span class="si">{</span><span class="n">var_Y_emp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Cov(X, Y): </span><span class="si">{</span><span class="n">cov_XY_th_ind</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Empirical Cov(X, Y): </span><span class="si">{</span><span class="n">cov_XY_emp_ind</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Var(X+Y): </span><span class="si">{</span><span class="n">var_sum_th_ind</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Empirical Var(X+Y): </span><span class="si">{</span><span class="n">var_sum_emp_ind</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Check: Var(X)+Var(Y) = </span><span class="si">{</span><span class="n">var_X_emp</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">var_Y_emp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Var(X-Y): </span><span class="si">{</span><span class="n">var_diff_th_ind</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Empirical Var(X-Y): </span><span class="si">{</span><span class="n">var_diff_emp_ind</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Check: Var(X)+Var(Y) = </span><span class="si">{</span><span class="n">var_X_emp</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">var_Y_emp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># Should match Var(X-Y) for independent</span>


<span class="c1"># Case 2: Dependent Variables (Positively Correlated)</span>
<span class="c1"># Create Y based on X to induce correlation</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span> <span class="c1"># Noise</span>
<span class="n">Y_dep</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X_ind</span> <span class="o">+</span> <span class="n">Z</span> <span class="o">+</span> <span class="mi">5</span> <span class="c1"># Y depends on X</span>

<span class="c1"># Calculate empirical covariance and theoretical variance for Y_dep</span>
<span class="n">cov_XY_emp_dep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_ind</span><span class="p">,</span> <span class="n">Y_dep</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">var_Y_emp_dep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">Y_dep</span><span class="p">)</span>
<span class="c1"># Theoretical Var(Y_dep) = Var(0.5*X_ind + Z + 5) = Var(0.5*X_ind + Z) assuming X_ind and Z are independent</span>
<span class="c1"># = (0.5)^2 * Var(X_ind) + Var(Z) = 0.25 * 4 + (1.5)^2 = 1 + 2.25 = 3.25</span>
<span class="n">var_Y_th_dep</span> <span class="o">=</span> <span class="mf">3.25</span>
<span class="c1"># Theoretical Cov(X, Y_dep) = Cov(X_ind, 0.5*X_ind + Z + 5)</span>
<span class="c1"># = Cov(X_ind, 0.5*X_ind) + Cov(X_ind, Z) + Cov(X_ind, 5)</span>
<span class="c1"># = 0.5 * Cov(X_ind, X_ind) + 0 + 0 = 0.5 * Var(X_ind) = 0.5 * 4 = 2</span>
<span class="n">cov_XY_th_dep</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Theoretical values for sums/differences</span>
<span class="n">var_sum_th_dep</span> <span class="o">=</span> <span class="n">var_X_th</span> <span class="o">+</span> <span class="n">var_Y_th_dep</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">cov_XY_th_dep</span>
<span class="n">var_diff_th_dep</span> <span class="o">=</span> <span class="n">var_X_th</span> <span class="o">+</span> <span class="n">var_Y_th_dep</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">cov_XY_th_dep</span>

<span class="c1"># Empirical values for sums/differences</span>
<span class="n">var_sum_emp_dep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X_ind</span> <span class="o">+</span> <span class="n">Y_dep</span><span class="p">)</span>
<span class="n">var_diff_emp_dep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X_ind</span> <span class="o">-</span> <span class="n">Y_dep</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Dependent Case (Positive Correlation) ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Empirical Var(X): </span><span class="si">{</span><span class="n">var_X_emp</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># Same X as before</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Var(Y_dep): </span><span class="si">{</span><span class="n">var_Y_th_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Empirical Var(Y_dep): </span><span class="si">{</span><span class="n">var_Y_emp_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Cov(X, Y_dep): </span><span class="si">{</span><span class="n">cov_XY_th_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Empirical Cov(X, Y_dep): </span><span class="si">{</span><span class="n">cov_XY_emp_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Var(X+Y_dep): </span><span class="si">{</span><span class="n">var_sum_th_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Empirical Var(X+Y_dep): </span><span class="si">{</span><span class="n">var_sum_emp_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Check: Var(X)+Var(Y)+2Cov(X,Y) = </span><span class="si">{</span><span class="n">var_X_emp</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">var_Y_emp_dep</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="o">*</span><span class="n">cov_XY_emp_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Theoretical Var(X-Y_dep): </span><span class="si">{</span><span class="n">var_diff_th_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Empirical Var(X-Y_dep): </span><span class="si">{</span><span class="n">var_diff_emp_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Check: Var(X)+Var(Y)-2Cov(X,Y) = </span><span class="si">{</span><span class="n">var_X_emp</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">var_Y_emp_dep</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span><span class="o">*</span><span class="n">cov_XY_emp_dep</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Independent Case ---
Theoretical Var(X): 4.0000, Empirical Var(X): 4.0113
Theoretical Var(Y): 9.0000, Empirical Var(Y): 8.9801
Theoretical Cov(X, Y): 0.0000, Empirical Cov(X, Y): 0.0392
Theoretical Var(X+Y): 13.0000, Empirical Var(X+Y): 13.0699
Check: Var(X)+Var(Y) = 12.9915
Theoretical Var(X-Y): 13.0000, Empirical Var(X-Y): 12.9131
Check: Var(X)+Var(Y) = 12.9915

--- Dependent Case (Positive Correlation) ---
Empirical Var(X): 4.0113
Theoretical Var(Y_dep): 3.2500, Empirical Var(Y_dep): 3.2587
Theoretical Cov(X, Y_dep): 2.0000, Empirical Cov(X, Y_dep): 2.0126
Theoretical Var(X+Y_dep): 11.2500, Empirical Var(X+Y_dep): 11.2953
Check: Var(X)+Var(Y)+2Cov(X,Y) = 11.2953
Theoretical Var(X-Y_dep): 3.2500, Empirical Var(X-Y_dep): 3.2448
Check: Var(X)+Var(Y)-2Cov(X,Y) = 3.2447
</pre></div>
</div>
</div>
</div>
<p>The simulation results closely match the theoretical calculations based on the variance formulas, both for independent and dependent variables. Notice how the positive covariance in the dependent case <em>increases</em> the variance of the sum and <em>decreases</em> the variance of the difference compared to the independent case.</p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>This chapter introduced key concepts for understanding the relationships between random variables:</p>
<ul class="simple">
<li><p><strong>Independence:</strong> Variables are independent if knowing the value of one provides no information about the other. Mathematically, their joint distribution function factorizes into the product of their marginals.</p></li>
<li><p><strong>Covariance:</strong> Measures the direction of the <em>linear</em> relationship (<span class="math notranslate nohighlight">\(E[XY] - E[X]E[Y]\)</span>). Positive covariance indicates variables tend to move together; negative indicates they move oppositely; zero indicates no linear association. Its magnitude depends on the variables’ units.</p></li>
<li><p><strong>Correlation Coefficient (<span class="math notranslate nohighlight">\(\rho\)</span>):</strong> A standardized measure (<span class="math notranslate nohighlight">\(\frac{\mathrm{Cov}(X, Y)}{\sigma_X \sigma_Y}\)</span>) of the strength and direction of the <em>linear</em> relationship, ranging from -1 (perfect negative linear) to +1 (perfect positive linear). <span class="math notranslate nohighlight">\(\rho=0\)</span> means no linear relationship, but not necessarily independence.</p></li>
<li><p><strong>Variance of Sums:</strong> The variance of a sum (or weighted sum) depends on the individual variances <em>and</em> the covariance between the variables: <span class="math notranslate nohighlight">\(\mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2 \mathrm{Cov}(X, Y)\)</span>. If the variables are independent, <span class="math notranslate nohighlight">\(\mathrm{Cov}(X,Y)=0\)</span>, simplifying the formula.</p></li>
</ul>
<p>We saw how to calculate covariance and correlation using NumPy and Pandas, visualize relationships using scatter plots, and verify the variance rules through simulation. These concepts are fundamental for multivariate statistics, machine learning (feature selection/engineering), finance (portfolio theory), and many other fields where understanding variable interactions is crucial.</p>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Conceptual:</strong> Give an example of two variables that you expect to be:
a) Positively correlated.
b) Negatively correlated.
c) Uncorrelated but dependent.
d) Independent.
Justify your reasoning.</p></li>
<li><p><strong>Calculation:</strong> Let <span class="math notranslate nohighlight">\(X\)</span> have <span class="math notranslate nohighlight">\(E[X]=2, \mathrm{Var}(X)=9\)</span>. Let <span class="math notranslate nohighlight">\(Y\)</span> have <span class="math notranslate nohighlight">\(E[Y]=-1, \mathrm{Var}(Y)=4\)</span>. Let <span class="math notranslate nohighlight">\(\mathrm{Cov}(X, Y) = -3\)</span>. Calculate:
a) <span class="math notranslate nohighlight">\(E[3X - 2Y + 5]\)</span>
b) <span class="math notranslate nohighlight">\(\mathrm{Var}(X + Y)\)</span>
c) <span class="math notranslate nohighlight">\(\mathrm{Var}(X - Y)\)</span>
d) <span class="math notranslate nohighlight">\(\mathrm{Var}(3X - 2Y + 5)\)</span>
e) <span class="math notranslate nohighlight">\(\rho(X, Y)\)</span></p></li>
<li><p><strong>Simulation (Correlation):</strong>
a) Generate 500 samples of <span class="math notranslate nohighlight">\(X \sim \text{Normal}(0, 1)\)</span>.
b) Generate 500 samples of <span class="math notranslate nohighlight">\(Y = -2X + \epsilon\)</span>, where <span class="math notranslate nohighlight">\(\epsilon \sim \text{Normal}(0, \sigma^2)\)</span> is noise. Try <span class="math notranslate nohighlight">\(\sigma = 0.5\)</span> and <span class="math notranslate nohighlight">\(\sigma = 2\)</span>.
c) For each value of <span class="math notranslate nohighlight">\(\sigma\)</span>, create a scatter plot of <span class="math notranslate nohighlight">\(X\)</span> vs <span class="math notranslate nohighlight">\(Y\)</span> and calculate the sample correlation coefficient <span class="math notranslate nohighlight">\(\rho(X, Y)\)</span>.
d) How does the noise level <span class="math notranslate nohighlight">\(\sigma\)</span> affect the correlation coefficient? Explain why.</p></li>
<li><p><strong>Simulation (Variance of Sums):</strong>
a) Let <span class="math notranslate nohighlight">\(X \sim \text{Poisson}(\lambda=3)\)</span> and <span class="math notranslate nohighlight">\(Y \sim \text{Poisson}(\lambda=5)\)</span>. Assume <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.
b) Generate 10,000 samples for <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.
c) Calculate the empirical variance of <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(Y\)</span>, and <span class="math notranslate nohighlight">\(X+Y\)</span>.
d) The theoretical variance of a Poisson(<span class="math notranslate nohighlight">\(\lambda\)</span>) distribution is <span class="math notranslate nohighlight">\(\lambda\)</span>. Compare the empirical <span class="math notranslate nohighlight">\(\mathrm{Var}(X+Y)\)</span> to the theoretical prediction <span class="math notranslate nohighlight">\(\mathrm{Var}(X) + \mathrm{Var}(Y)\)</span> (since they are independent). Are they close?
e) It’s a known property that the sum of independent Poisson variables is also Poisson, so <span class="math notranslate nohighlight">\(X+Y \sim \text{Poisson}(\lambda_X + \lambda_Y)\)</span>. What is the theoretical variance of <span class="math notranslate nohighlight">\(X+Y\)</span> based on this property? Does it match your findings?</p></li>
<li><p><strong>Pandas Correlation:</strong> Load a dataset (e.g., the <code class="docutils literal notranslate"><span class="pre">tips</span></code> dataset from Seaborn: <code class="docutils literal notranslate"><span class="pre">tips</span> <span class="pre">=</span> <span class="pre">sns.load_dataset('tips')</span></code>). Calculate the correlation matrix for the numerical columns (e.g., <code class="docutils literal notranslate"><span class="pre">total_bill</span></code>, <code class="docutils literal notranslate"><span class="pre">tip</span></code>, <code class="docutils literal notranslate"><span class="pre">size</span></code>). Interpret the correlation between <code class="docutils literal notranslate"><span class="pre">total_bill</span></code> and <code class="docutils literal notranslate"><span class="pre">tip</span></code>. Visualize this relationship with a scatter plot.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example code for Exercise 5 setup</span>
<span class="c1"># import seaborn as sns</span>
<span class="c1"># import pandas as pd</span>
<span class="c1"># tips = sns.load_dataset(&#39;tips&#39;)</span>
<span class="c1"># print(tips.head())</span>
<span class="c1"># numerical_tips = tips[[&#39;total_bill&#39;, &#39;tip&#39;, &#39;size&#39;]]</span>
<span class="c1"># correlation_matrix = numerical_tips.corr()</span>
<span class="c1"># print(&quot;\nCorrelation Matrix:&quot;)</span>
<span class="c1"># print(correlation_matrix)</span>
<span class="c1"># sns.scatterplot(data=tips, x=&#39;total_bill&#39;, y=&#39;tip&#39;)</span>
<span class="c1"># plt.title(&#39;Total Bill vs Tip Amount&#39;)</span>
<span class="c1"># plt.show()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="chapter_10.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 10: Joint Distributions</p>
      </div>
    </a>
    <a class="right-next"
       href="chapter_12.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 12: Functions of Multiple Random Variables</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-of-random-variables">Independence of Random Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-for-independence">Checking for Independence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-covariance-with-numpy">Calculating Covariance with NumPy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-coefficient">Correlation Coefficient</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-and-visualizing-correlation">Calculating and Visualizing Correlation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-correlation-between-study-hours-and-exam-scores">Example: Correlation between Study Hours and Exam Scores</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-of-sums-of-random-variables">Variance of Sums of Random Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hands-on-demonstrating-variance-rules-via-simulation">Hands-on: Demonstrating Variance Rules via Simulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>